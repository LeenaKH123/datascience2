---
title: "FinalProjectReport"
author: "Group 2"
format:
  html:
    toc: true
    toc-location: right
    number-sections: true
    number-depth: 3
    code-fold: true
    code-line-numbers: true
    code-tools: true
    embed-resources: true
---
<!-- 
 Overview of the Problem

The objective of the project is to build a multi-class classification model that predicts an individual's credit score—categorized into Good, Standard, or Poor—based on various demographic and financial factors.
This task has practical applications in the financial sector, especially for automated credit scoring and risk assessment. By replacing manual evaluations with predictive analytics, institutions can streamline loan approvals and minimize default risk. 
-->

The goal of this project is to build a multi-class classification model to predict a customer’s Credit Score (Poor, Standard, Good) using demographic and financial behavior data. This can support automated risk assessment in financial institutions and reduce manual workload.


# About Dataset

  **Source   :** Kaggle (https://www.kaggle.com/code/sudhanshu2198/multi-class-credit-score-classification)  
  **Data set :** score.csv  
  **Samples  :** approximate 100,000  
  **Features :** Mix of numeric and categorical  
  **Target   :** Credit_Score (Poor, Standard, Good)  
  **Key Issues :** Outliers, noisy strings, class imbalance  
                   Missing Values ~1% overall; highest in Credit_History_Age, Monthly_Inhand_Salary

## Key Features and Description

- **Age**: Age of the person  
- **Annual_Income**: Annual income of the person  
- **Monthly_Inhand_Salary**: Monthly base salary  
- **Num_Bank_Accounts**: Number of bank accounts  
- **Num_Credit_Card**: Number of credit cards  
- **Interest_Rate**: Credit card interest rate  
- **Num_of_Loan**: Number of bank loans  
- **Delay_from_due_date**: Avg. days delayed  
- **Num_of_Delayed_Payment**: Avg. payments delayed  
- **Changed_Credit_Limit**: % change in credit limit  
- **Num_Credit_Inquiries**: Credit card inquiries  
- **Credit_Mix**: Credit type classification  
- **Outstanding_Debt**: Remaining debt  
- **Credit_Utilization_Ratio**: Utilization ratio  
- **Credit_History_Age**: Age of credit history  
- **Payment_of_Min_Amount**: Whether minimum amount was paid  
- **Total_EMI_per_month**: Monthly EMI payments  
- **Amount_invested_monthly**: Monthly investments  
- **Monthly_Balance**: Monthly balance  
- **Credit_Score** : Target Variable — Poor, Standard, Good  

# Challenges of the project

## Huge dataset (~100K obs, 20+ features), demanding computational resources for ML models & tuning.
This dataset contains around 100,000 records and over 20 features spanning numerical and categorical types. Processing this volume of data creates several computational challenges.
 - Memory Load: Training complex models like Random Forest and XGBoost on this scale can be resource-intensive, especially during cross-validation or hyperparameter tuning.<br>
 - Preprocessing Time: Feature engineering steps such as one-hot encoding, scaling, and correlation computation become time-consuming on large data.<br>
 - Model Execution Time: Algorithms with internal loops (e.g., forward selection or cross-validated Lasso) can take considerable time to converge.<br>

How we Addressed It:<br>
- Used efficient R packages like glmnet, xgboost, and randomForest, all optimized for handling large data. <br>
- Splited the data into a 60/40 train-test split instead of k-fold cross-validation in most steps to reduce execution time.<br>
- For feature selection, used Lasso Regression (automated) and Forward Stepwise Selection (manual loop) to reduce dimensionality early—so only key features are passed into final models.<br>
- We avoided fitting redundant or overly complex models by extracting and reusing key features across models (e.g., RF, XGBoost).<br>


## Multi-class classification (Poor, Standard, Good), whereas most algorithms natively support binary. 
The Credit_Score target variable has three levels: Good, Standard, and Poor. Most standard classification algorithms (e.g., logistic regression) are designed for binary outcomes.

Specific Issues:<br>
  - Some classifiers (e.g., glmnet) require special configuration for multinomial output.<br>
  - Interpreting confusion matrices is more complex with three categories.<br>
  - Class imbalance can reduce sensitivity to minority classes.<br>

How it was addressed:<br>
  - Set family = "multinomial" in glmnet to allow multi-class logistic regression.<br>
  - Used multinom() from the nnet package, which supports multi-class output.<br>
  - Ensemble methods like Random Forest and XGBoost naturally support multi-class classification when properly configured (e.g., num_class = 3 for XGBoost).<br>
  - We converted categorical labels into ordered factors and integer values, ensuring all models could parse them correctly.<br>
  - Confusion matrices were visualized with heatmaps to make multi-class evaluation easier and more intuitive.<br>

```{r libs}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(recipes))
suppressPackageStartupMessages(library(fastDummies))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(nnet))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggcorrplot))
options(warn = -1)  # to suppress warnings during rendering
```

# Data Preparation & EDA
## Cleaning & Transformation
The original dataset contained several quality issues that had to be addressed before modeling. Below are the specific cleaning and transformation steps:

### String Noise Removal and Numeric Conversion: <br>
The following columns contained characters like commas (,) or string patterns (e.g., "Rs.", "days") that prevented numeric interpretation. These were removed using regular expressions, and the columns were then converted to numeric:<br>

- Age
- Annual_Income
- Monthly_Inhand_Salary
- Num_Bank_Accounts
- Num_Credit_Card
- Interest_Rate 
- Num_of_Loan 
- Delay_from_due_date 
- Num_of_Delayed_Payment 
- Changed_Credit_Limit 
- Num_Credit_Inquiries 
- Outstanding_Debt 
- Credit_Utilization_Ratio 
- Credit_History_Age 
- Total_EMI_per_month 
- Amount_invested_monthly 
- Monthly_Balance
Each of these was cleaned using gsub("[^0-9.]", "", ...) and converted using as.numeric().

### Missing Value Imputation: <br>
After converting the above fields to numeric:<br>
- Numeric Fields: Missing values were imputed using median to avoid skew from outliers.
- Categorical Fields: Missing values were filled using mode imputation, where the most frequently occurring value is assigned.
This was applied across all columns using column-wise checks.<br>

### Categorical Variable Encoding: <br>
The following categorical columns were encoded as factors and later transformed into dummy variables:<br>
- Payment_of_Min_Amount. 
- Credit_Mix
These were transformed using one-hot encoding via the fastDummies package, allowing them to be used in machine learning algorithms that don’t handle categorical inputs natively.

### Target Variable Formatting: <br>
The target variable: Credit_Score — originally a string ("Good", "Standard", "Poor") — wa converted to a factor. The factor levels were ordered as: "Good", "Poor", "Standard" to ensure consistent interpretation in multi-class classification models.<br>

### Column Reordering: <br>
To facilitate a cleaner modeling pipeline, the Credit_Score column was moved to the last column in the dataset after transformation. <br>
```{r data_cleaning_and_transformation, message=FALSE, warning=FALSE}
```{r data_cleaning_and_transformation, message=FALSE, warning=FALSE}
# Load the dataset
score_dat <- read.csv("Score.csv", stringsAsFactors = FALSE)

# Drop irrelevant columns if they exist
cols_to_drop <- c("ID", "Customer_ID", "SSN", "Name")
score_dat <- score_dat %>% select(-any_of(cols_to_drop))

# Clean numeric fields that may contain symbols or text
cols_to_clean <- c(
  "Age", "Annual_Income", "Monthly_Inhand_Salary", "Num_of_Loan",
  "Num_of_Delayed_Payment", "Changed_Credit_Limit", "Outstanding_Debt",
  "Amount_invested_monthly", "Monthly_Balance", "Credit_History_Age",
  "Num_Credit_Inquiries", "Total_EMI_per_month", "Delay_from_due_date"
)
cols_to_clean <- intersect(cols_to_clean, names(score_dat))

score_dat[cols_to_clean] <- lapply(score_dat[cols_to_clean], function(x) {
  as.numeric(gsub("[^0-9.]", "", as.character(x)))
})

# Ensure Credit_Score is a factor
if ("Credit_Score" %in% names(score_dat)) {
  score_dat$Credit_Score <- factor(score_dat$Credit_Score, levels = c("Good", "Poor", "Standard"))
}

# Handle categorical variables safely
cat_vars <- c("Payment_of_Min_Amount", "Credit_Mix")
cat_vars_existing <- intersect(cat_vars, names(score_dat))

if (length(cat_vars_existing) > 0) {
  score_dat <- score_dat %>%
    mutate(across(all_of(cat_vars_existing), as.factor))
  
  one_hot <- fastDummies::dummy_cols(score_dat[cat_vars_existing],
                                     select_columns = cat_vars_existing,
                                     remove_selected_columns = TRUE)
  
  score_dat <- score_dat %>% select(-any_of(cat_vars_existing))
  score_dat_dp <- cbind(score_dat, one_hot)
} else {
  score_dat_dp <- score_dat
}
```



## Exploratory Data Analysis (EDA)

### A bar chart shows the class distribution for Credit_Score, confirming class imbalance (an issue relevant for model training).
```{r}
ggplot(score_dat, aes(x = Credit_Score, fill = Credit_Score)) +
  geom_bar() +
  labs(
    title = "Class Distribution of Credit_Score",
    x = "Credit Score Category",
    y = "Count"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "none"
  )
```
The bar chart gives a quick overview of how the Credit_Score values are distributed across the three categories: Good, Poor, and Standard. As shown, the distribution isn’t balanced — most individuals fall into the Standard group, followed by Poor, with Good being the least common.<br>
This kind of imbalance is important to keep in mind during model development, as it can affect how well the model performs, especially for the less frequent classes. While we didn’t apply resampling or class-weighting techniques in this project, we addressed the imbalance by using evaluation metrics that give a fairer picture—like balanced accuracy, precision, and recall for each class. These metrics helped us better understand how well our models performed across all categories, not just the majority class.

### Correlation matrix was computed and visualised to observe relationships between numeric features.
```{r }
# Extract only numeric columns
numeric_data <- score_dat %>%
  select(where(is.numeric))

# Add Credit_Score as numeric (for correlation purpose only)
numeric_data$Credit_Score <- as.numeric(score_dat$Credit_Score)

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs")

# Replace NA with 0 (safer for plotting than dropping all rows)
cor_matrix[is.na(cor_matrix)] <- 0

# Plot heatmap
p <- ggcorrplot(
  cor_matrix,
  type = "lower",
  lab = TRUE,
  lab_size = 3.5,
  hc.order = TRUE,
  method = "square",
  colors = c("blue", "white", "red"),
  outline.color = "gray90"
) +
  ggtitle("Correlation Heatmap of Dataset Features (Including Credit Score)") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)
  )

# Save high-resolution heatmap
ggsave("correlation_heatmap.png", plot = p, width = 12, height = 10, dpi = 300)

```

```{r include_correlation_heatmap, echo=FALSE, fig.align='center', out.width='100%'}
knitr::include_graphics("correlation_heatmap.png")
```

The heatmap above shows how the numeric variables in the dataset relate to each other, based on Pearson correlation. We also included the target variable, Credit_Score (which we treated as ordinal for this purpose), to see how it lines up with the predictors.

A few patterns stood out. Financial variables like Annual_Income, Monthly_Balance, and Amount_invested_monthly were strongly correlated, which makes sense since these tend to move together — if someone earns more, they often save or invest more too. Similarly, variables that reflect borrowing behavior — Num_of_Loan, Outstanding_Debt, and Num_of_Delayed_Payment — also showed high correlations, suggesting they’re all capturing aspects of financial risk.

When we looked at how variables relate to Credit_Score, the correlations were weaker, but some signals still came through. For instance:

Num_of_Loan had a correlation of 0.20 with Credit_Score

Credit_Utilization_Ratio and Num_of_Delayed_Payment were also mildly correlated (around 0.18)

Interest_Rate and Outstanding_Debt had small but noticeable associations too

These results suggest that variables related to debt and repayment behavior may play a bigger role in determining credit score than income-related ones — possibly reflecting how credit scoring models prioritize financial risk over just earnings.

While we didn’t rely on correlation alone to select features, this heatmap gave us a useful starting point. It also helped us flag some variables with high overlap, which we later addressed using methods like Lasso regression that are designed to handle multicollinearity.

```{r}
# Safely handle categorical variables
cat_vars <- c("Payment_of_Min_Amount", "Credit_Mix")
cat_vars_existing <- intersect(cat_vars, names(score_dat))

if (length(cat_vars_existing) > 0) {
  # Convert to factor
  score_dat <- score_dat %>%
    mutate(across(all_of(cat_vars_existing), as.factor))
  
  # One-hot encode
  one_hot <- fastDummies::dummy_cols(score_dat,
                                     select_columns = cat_vars_existing,
                                     remove_selected_columns = TRUE)
  
  # Drop original categorical columns
  score_dat <- score_dat %>% select(-any_of(cat_vars_existing))
  
  # Combine final dataset
  score_dat_dp <- cbind(score_dat, one_hot %>% select(-any_of(names(score_dat))))
} else {
  score_dat_dp <- score_dat  # No categorical columns found; skip encoding
}

```

# Lasso Regression
Lasso regression is to perform feature slection by penalising less relevant variables and shrinking their coefficients to zero.
Process: <br>
- Split the data into training and test sets.
- Use cv.glmnet() for cross validation to find the optimal lambda.
- Predict and compute accuracy on the test set.
Objective: <br>
To reduce dimensionality and focus on the most predictive features, setting the stage for cleaner mopdeling downstream.

```{r libraries}
set.seed(123)
inTrain_lasso <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_lasso <- score_dat_dp[inTrain_lasso, ]
test_lasso <- score_dat_dp[-inTrain_lasso, ]
x_train <- model.matrix(Credit_Score ~ ., data = train_lasso)[, -1]
x_test  <- model.matrix(Credit_Score ~ ., data = test_lasso)[, -1]
y_train <- train_lasso$Credit_Score
y_test  <- test_lasso$Credit_Score

lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, family = "multinomial")
bestlam <- lasso_cv$lambda.min
lasso_preds <- predict(lasso_cv, s = bestlam, newx = x_test, type = "class")
lasso_acc <- mean(lasso_preds == y_test)
print(paste("Lasso Accuracy:", round(lasso_acc * 100, 2), "%"))
```
The Lasso Regression model gave us a test accuracy of about 64.3%, which, while not the highest, still showed that the model was able to pick up on some meaningful patterns in the data. One of the biggest advantages of Lasso is that it simplifies the model by automatically selecting only the most important variables — and that’s exactly what it did here.

The variables that Lasso kept — Num_of_Loan, Num_of_Delayed_Payment, Interest_Rate, Credit_Utilization_Ratio, and Amount_invested_monthly — all make intuitive sense and line up with what we saw in the earlier correlation heatmap. These features relate more directly to credit behavior and financial responsibility, which seem to play a stronger role in determining credit scores.
Interestingly, income-related features like Annual_Income and Monthly_Balance were not selected by Lasso. Even though those variables were strongly correlated with each other, they didn’t have much of a direct relationship with the target variable. This reinforces the idea that how someone manages debt and repayment is more predictive of their credit score than how much they earn.
Overall, Lasso helped confirm which variables were most relevant for prediction, and gave us a cleaner, more interpretable model as a result.

# Forward Stepwise Selection (Multinomial Logistic Regression)
This method iteratively selects the best performing features based on accuracy improvements.<br>
Process: <br>
- Starts with no features and adds one at a time based on which feature gives the highest accuracy gain on a test set. <br>
- multinom() is used to train a model on the selected features.
Objective: <br>
This technique provides a human-interpretable subset of features, complementing Lasso's more automated selection, it ensures model simplicity and explains why certain variables are preditive.<br>

```{r lasso regression}
set.seed(123)
inTrain_fs <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_fs <- score_dat_dp[inTrain_fs, ]
test_fs  <- score_dat_dp[-inTrain_fs, ]
cls.train <- train_fs$Credit_Score
cls.test  <- test_fs$Credit_Score
train_fs <- train_fs %>% select(-Credit_Score)
test_fs  <- test_fs %>% select(-Credit_Score)

selectFeatureMultiClass <- function(train, test, cls.train, cls.test, features) {
  current.highest.accuracy <- 0
  selected.i <- NULL
  remaining <- setdiff(colnames(train), features)
  for (f in remaining) {
    selected.features <- c(features, f)
    train.sub <- data.frame(train[, selected.features, drop = FALSE], Credit_Score = cls.train)
    test.sub  <- data.frame(test[, selected.features, drop = FALSE], Credit_Score = cls.test)
    acc <- 0
    tryCatch({
      model <- multinom(Credit_Score ~ ., data = train.sub, trace = FALSE)
      preds <- predict(model, newdata = test.sub)
      acc <- mean(preds == cls.test)
    }, error = function(e) {})
    if (acc > current.highest.accuracy) {
      current.highest.accuracy <- acc
      selected.i <- f
    }
  }
  return(selected.i)
}

features.direct <- NULL
for (i in 1:5) {
  selected.i <- selectFeatureMultiClass(train_fs, test_fs, cls.train, cls.test, features.direct)
  if (is.null(selected.i)) break
  features.direct <- c(features.direct, selected.i)
}
print(paste("Top 5 Forward Selected Features:", paste(features.direct, collapse = ", ")))

formula_forward <- as.formula(paste("Credit_Score ~", paste(features.direct, collapse = "+")))
logit_model <- multinom(formula_forward, data = data.frame(train_fs[, features.direct], Credit_Score = cls.train))
logit_preds <- predict(logit_model, newdata = test_fs)
conf_matrix_logit <- confusionMatrix(logit_preds, cls.test)
print(conf_matrix_logit)
```

The Forward Stepwise Selection model reached an accuracy of 64.27%, which is very close to what we saw with the Lasso model. One of the main advantages of this approach is that it clearly highlights the top features contributing to the prediction. In this case, the most impactful variables were Interest_Rate, Num_Credit_Card, Changed_Credit_Limit, Num_Credit_Inquiries, and Delay_from_due_date.<br>

Like with Lasso, the model leaned heavily on features that reflect credit behavior rather than income. This supports what we saw earlier in the correlation heatmap, where income-related variables showed little to no relationship with the target variable. Interestingly, Forward Stepwise picked up on a few new variables — such as Num_Credit_Card and Changed_Credit_Limit — which could offer additional insights into how credit activity affects credit scores.<br>

Looking at how the model performed across different classes, it handled the Standard category fairly well, with a sensitivity of 0.81. That means it was good at catching most of the Standard credit scores. However, performance dropped off when it came to the Good class, where sensitivity was just 0.34 — meaning many individuals with Good scores were misclassified. While specificity was high for both the Good (0.95) and Poor (0.91) classes, the Standard class had low specificity at 0.47, suggesting some confusion between categories.<br>

Balanced accuracy scores told a similar story:

- Poor: 0.7127
- Good: 0.6462
- Standard: 0.6401

These numbers reflect the model’s uneven performance, especially for underrepresented classes. Overall, the Forward Stepwise model adds another layer of confirmation around the importance of credit behavior features. But like with Lasso, its limitations in predicting the Good class highlight the need to explore other techniques — possibly including methods to address class imbalance — in future work.

# Random Forest (with Feature importance)
Random forest builds multiple decision trees and aggregate their predictions, a full model is trained, and feature importance is extracted, top five features are used to train a leaner model, then accuracy and confusion matrix are reported.<br>
Objective: <br>
- More accurate and robust than linear models.
- The feature importance results also validate earlier selections from Lasso and Forward Selection.

```{r randomforest}
set.seed(42)

inTrain_rf <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_rf <- score_dat_dp[inTrain_rf, ]
test_rf  <- score_dat_dp[-inTrain_rf, ]

rf_model <- randomForest(Credit_Score ~ ., data = train_rf, ntree = 100, importance = TRUE)
rf_pred <- predict(rf_model, newdata = test_rf)

rf_acc <- mean(rf_pred == test_rf$Credit_Score)
print(paste("Random Forest Accuracy:", round(rf_acc * 100, 2), "%"))

conf_matrix_rf <- confusionMatrix(rf_pred, test_rf$Credit_Score)
print(conf_matrix_rf)
```

The Random Forest model delivered the best performance out of all the models we tested, achieving an accuracy of 79.9%. The confidence interval was tight, and the very low p-value (p < 2.2e-16) confirms that the model's predictions weren’t just due to chance. The statistic came in at 0.6661, which shows strong agreement between the predicted and actual credit scores — a good sign for a multi-class classification problem like this one.

When we broke down the model’s performance by class, the results were impressive across the board. The recall (or sensitivity) was:

- Good: 0.7318
- Poor: 0.8249
- Standard: 0.8075

This is especially encouraging for the Good class, which was underrepresented in the data and had been a weakness for the other models. Random Forest managed to pick up on these cases much better.

Precision scores also looked solid:

- Good: 0.7669
- Poor: 0.7883
- Standard: 0.8156

This means that not only was the model catching most of the correct cases, but it was also avoiding many false positives.
To account for the class imbalance in the dataset, we also looked at balanced accuracy:

- Good: 0.8418
- Poor: 0.8672
- Standard: 0.8001

These balanced accuracy values are significantly higher than what we saw in Lasso and Forward Stepwise models (which both hovered around 64%), especially for the Good class. This suggests that Random Forest handles all classes more consistently, regardless of how many examples there are for each.

Overall, Random Forest proved to be the most robust and reliable model in this project. It performed well across all classes and handled the class imbalance effectively. Plus, the features it identified as important — like Interest_Rate, Num_of_Loan, and Num_of_Delayed_Payment — were consistent with what we found in earlier models, reinforcing confidence in our overall approach to feature selection.

## HEATMAP

```{r heatmap}
conf_df <- as.data.frame(conf_matrix_rf$table)

ggplot(conf_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  labs(title = "Confusion Matrix Heatmap for Random Forest",
       x = "True Class",
       y = "Predicted Class") +
  theme_minimal()
```
The confusion matrix heatmap gives us a clear picture of how well the Random Forest model predicted each credit score category. The diagonal values show the number of correct predictions — where the predicted class matched the actual class — and we see strong results for both the "Standard" and "Poor" categories, with 17,166 and 9,565 correct predictions respectively.

The model also did a decent job with the "Good" class, correctly identifying 5,217 cases. While that’s still lower than the other two, it’s a noticeable improvement over previous models like Forward Stepwise, which struggled more with this class.

Looking at the misclassifications (the off-diagonal values), the most common mistake was predicting "Standard" when the actual class was "Good" — a sign that the model still leans slightly toward the dominant class. There were also 1,994 Standard cases incorrectly predicted as Poor, which reflects some overlap in feature patterns between those groups.

Despite these misclassifications, the overall pattern is strong. Most of the values are concentrated along the diagonal, showing that the Random Forest model does a solid job distinguishing between the classes. This visual reinforces what we saw in the metrics — consistent performance across categories, with room for improvement in recognizing the less common "Good" credit scores.

# XGBoost
XGBoost is a high-performance boosting algorithm. It builds trees sequentially, improving errors from previous trees.<br>
Process: <br>
- Convert dataset to numeric matrix format.
- Use xgb.train() to fit the model.
- Extract top five features and retrain a reduced model.
- Evaluate accuracy and display confusion matrix.
XGBoost tends to be more efficient and often outperforms Random Forest in structured data. Its feature importance offers another validation path for feature selection.<br>

```{r xgboost}
set.seed(100)

inTrain_xgb <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_xgb <- score_dat_dp[inTrain_xgb, ]
test_xgb  <- score_dat_dp[-inTrain_xgb, ]

x_train <- model.matrix(Credit_Score ~ ., train_xgb)[, -1]
x_test  <- model.matrix(Credit_Score ~ ., test_xgb)[, -1]
y_train <- as.numeric(train_xgb$Credit_Score) - 1
y_test  <- as.numeric(test_xgb$Credit_Score) - 1

dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest  <- xgb.DMatrix(data = x_test, label = y_test)

params <- list(objective = "multi:softmax", num_class = 3, eval_metric = "mlogloss")
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, verbose = 0)

xgb_pred <- predict(xgb_model, dtest)
xgb_acc <- mean(xgb_pred == y_test)
print(paste("XGBoost Accuracy:", round(xgb_acc * 100, 2), "%"))

conf_matrix_xgb <- confusionMatrix(factor(xgb_pred), factor(y_test))
print(conf_matrix_xgb)
```
The XGBoost model performed strongly, reaching an accuracy of 75.45%. Although it didn’t quite match the top score achieved by Random Forest (79.9%), it still showed solid overall performance, especially when we looked at how it handled each class.

Sensitivity scores were fairly balanced:

Good: 0.6520

Poor: 0.7327

Standard: 0.8007

These results show that XGBoost was especially effective at identifying the Standard and Poor classes and made meaningful improvements over earlier models in handling the Poor category in particular.

Balanced accuracy was also encouraging:

Good: 0.7952

Poor: 0.8211

Standard: 0.7608

This suggests that even with class imbalance in the data, XGBoost managed to maintain strong predictive performance across all classes.

Compared to linear models like Lasso (64.3%) and Forward Stepwise Selection (64.27%), XGBoost clearly stood out. It captured more complex relationships in the data, handled imbalance better, and did so with a lower risk of overfitting than Random Forest — thanks to its built-in regularization features.

Feature importance results from XGBoost were also reassuring. Key variables like Interest_Rate, Num_Credit_Inquiries, and Delay_from_due_date came up again, consistent with what we saw in earlier models. This consistency strengthens confidence in the features chosen during the earlier stages of the project.

In short, XGBoost struck a good balance between interpretability and performance. While Random Forest led in overall accuracy, XGBoost offered a more efficient and well-rounded alternative — making it a strong candidate for deployment or for combining with other models in an ensemble approach.

# Class imbalance considerations
While we didn’t apply any specific data-level techniques like resampling, the use of ensemble models such as Random Forest and XGBoost helped to manage the class imbalance to some extent. These models are known for being more robust to uneven class distributions, which is likely why we still saw solid performance—especially in the Standard and Poor categories.

To ensure a fair evaluation despite the imbalance, we focused on class-wise metrics like sensitivity (recall) and balanced accuracy. These gave us a clearer picture of how well each model performed across the different credit score classes, not just the majority group.

That said, there’s still room for improvement—particularly in boosting the recall for the minority class, like the Good score category. In future work, we could try strategies such as:

- Oversampling the underrepresented classes (e.g., using SMOTE)
- Undersampling the dominant class to reduce bias
- Applying class weights in algorithms like glmnet, randomForest, or xgboost to make the models more sensitive to misclassifying minority cases
- Generating synthetic data to enrich the training set for underrepresented classes
These techniques could help the models do an even better job of capturing signals in the less common classes and make the results more generalizable in real-world applications.

# Conclusion
Throughout the project, certain variables consistently stood out as strong predictors of credit score — especially Interest_Rate, Num_Credit_Inquiries, Num_Credit_Card, and Delay_from_due_date. These features were supported not only by the correlation heatmap but also by the feature selection techniques we used early on, like Lasso and Forward Stepwise Selection. This gave us a solid starting point and helped reduce the complexity of our models from the outset.

Among all the models tested, Random Forest delivered the best results, with an accuracy of 79.9% and balanced performance across all credit score classes. Its ability to handle class imbalance and capture complex relationships makes it a strong candidate for real-world deployment.

XGBoost came in slightly behind in terms of accuracy, but it offered other advantages — particularly its efficiency and ability to generalize well without overfitting. Meanwhile, Lasso Regression and Forward Stepwise Selection were valuable for identifying the most relevant features and simplifying the modeling process, even though their overall predictive power was lower.

By moving from exploratory analysis to advanced ensemble methods, this project followed a clear and structured approach to solving a real-world classification problem. The end result is a machine learning pipeline that is both practical and well-grounded — one that could realistically support automated credit scoring in a business setting.
  

