---
title: "FinalProjectReport"
author: "Group 2"
format:
    html:
      code-fold: true
      code-line-numbers: true
      number-sections: true
      number-depth: 3
      code-tools: true
      embed-resources: true
---

<!-- 
 Overview of the Problem

The objective of the project is to build a multi-class classification model that predicts an individual's credit score—categorized into Good, Standard, or Poor—based on various demographic and financial factors.
This task has practical applications in the financial sector, especially for automated credit scoring and risk assessment. By replacing manual evaluations with predictive analytics, institutions can streamline loan approvals and minimize default risk. 
-->

The goal of this project is to build a multi-class classification model to predict a customer’s Credit Score (Poor, Standard, Good) using demographic and financial behavior data. This can support automated risk assessment in financial institutions and reduce manual workload.


# About Dataset

  **Source   :** Kaggle (https://www.kaggle.com/code/sudhanshu2198/multi-class-credit-score-classification)  
  **Data set :** score.csv  
  **Samples  :** approximate 100,000  
  **Features :** Mix of numeric and categorical  
  **Target   :** Credit_Score (Poor, Standard, Good)  
  **Key Issues :** Outliers, noisy strings, class imbalance  
                   Missing Values ~1% overall; highest in Credit_History_Age, Monthly_Inhand_Salary

## Key Features and Description

- **Age**: Age of the person  
- **Annual_Income**: Annual income of the person  
- **Monthly_Inhand_Salary**: Monthly base salary  
- **Num_Bank_Accounts**: Number of bank accounts  
- **Num_Credit_Card**: Number of credit cards  
- **Interest_Rate**: Credit card interest rate  
- **Num_of_Loan**: Number of bank loans  
- **Delay_from_due_date**: Avg. days delayed  
- **Num_of_Delayed_Payment**: Avg. payments delayed  
- **Changed_Credit_Limit**: % change in credit limit  
- **Num_Credit_Inquiries**: Credit card inquiries  
- **Credit_Mix**: Credit type classification  
- **Outstanding_Debt**: Remaining debt  
- **Credit_Utilization_Ratio**: Utilization ratio  
- **Credit_History_Age**: Age of credit history  
- **Payment_of_Min_Amount**: Whether minimum amount was paid  
- **Total_EMI_per_month**: Monthly EMI payments  
- **Amount_invested_monthly**: Monthly investments  
- **Monthly_Balance**: Monthly balance  
- **Credit_Score** : Target Variable — Poor, Standard, Good  

# Challenges of the project

## Huge dataset (~100K obs, 20+ features), demanding computational resources for ML models & tuning.
This dataset contains around 100,000 records and over 20 features spanning numerical and categorical types. Processing this volume of data creates several computational challenges.
 - Memory Load: Training complex models like Random Forest and XGBoost on this scale can be resource-intensive, especially during cross-validation or hyperparameter tuning.<br>
 - Preprocessing Time: Feature engineering steps such as one-hot encoding, scaling, and correlation computation become time-consuming on large data.<br>
 - Model Execution Time: Algorithms with internal loops (e.g., forward selection or cross-validated Lasso) can take considerable time to converge.<br>

How we Addressed It:<br>
- Used efficient R packages like glmnet, xgboost, and randomForest, all optimized for handling large data. <br>
- Splited the data into a 60/40 train-test split instead of k-fold cross-validation in most steps to reduce execution time.<br>
- For feature selection, used Lasso Regression (automated) and Forward Stepwise Selection (manual loop) to reduce dimensionality early—so only key features are passed into final models.<br>
- We avoided fitting redundant or overly complex models by extracting and reusing key features across models (e.g., RF, XGBoost).<br>


## Multi-class classification (Poor, Standard, Good), whereas most algorithms natively support binary. 
The Credit_Score target variable has three levels: Good, Standard, and Poor. Most standard classification algorithms (e.g., logistic regression) are designed for binary outcomes.

Specific Issues:<br>
  - Some classifiers (e.g., glmnet) require special configuration for multinomial output.<br>
  - Interpreting confusion matrices is more complex with three categories.<br>
  - Class imbalance can reduce sensitivity to minority classes.<br>

How it was addressed:<br>
  - Set family = "multinomial" in glmnet to allow multi-class logistic regression.<br>
  - Used multinom() from the nnet package, which supports multi-class output.<br>
  - Ensemble methods like Random Forest and XGBoost naturally support multi-class classification when properly configured (e.g., num_class = 3 for XGBoost).<br>
  - We converted categorical labels into ordered factors and integer values, ensuring all models could parse them correctly.<br>
  - Confusion matrices were visualized with heatmaps to make multi-class evaluation easier and more intuitive.<br>

```{r libs}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(recipes))
suppressPackageStartupMessages(library(fastDummies))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(nnet))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggcorrplot))
options(warn = -1)  # to suppress warnings during rendering
```

# Data Preparation & EDA
## Cleaning & Transformation
The original dataset contained several quality issues that had to be addressed before modeling. Below are the specific cleaning and transformation steps:

### String Noise Removal and Numeric Conversion: <br>
The following columns contained characters like commas (,) or string patterns (e.g., "Rs.", "days") that prevented numeric interpretation. These were removed using regular expressions, and the columns were then converted to numeric:<br>

- Age
- Annual_Income
- Monthly_Inhand_Salary
- Num_Bank_Accounts
- Num_Credit_Card
- Interest_Rate 
- Num_of_Loan 
- Delay_from_due_date 
- Num_of_Delayed_Payment 
- Changed_Credit_Limit 
- Num_Credit_Inquiries 
- Outstanding_Debt 
- Credit_Utilization_Ratio 
- Credit_History_Age 
- Total_EMI_per_month 
- Amount_invested_monthly 
- Monthly_Balance
Each of these was cleaned using gsub("[^0-9.]", "", ...) and converted using as.numeric().

### Missing Value Imputation: <br>
After converting the above fields to numeric:<br>
- Numeric Fields: Missing values were imputed using median to avoid skew from outliers.
- Categorical Fields: Missing values were filled using mode imputation, where the most frequently occurring value is assigned.
This was applied across all columns using column-wise checks.<br>

### Categorical Variable Encoding: <br>
The following categorical columns were encoded as factors and later transformed into dummy variables:<br>
- Payment_of_Min_Amount. 
- Credit_Mix
These were transformed using one-hot encoding via the fastDummies package, allowing them to be used in machine learning algorithms that don’t handle categorical inputs natively.

### Target Variable Formatting: <br>
The target variable: Credit_Score — originally a string ("Good", "Standard", "Poor") — wa converted to a factor. The factor levels were ordered as: "Good", "Poor", "Standard" to ensure consistent interpretation in multi-class classification models.<br>

### Column Reordering: <br>
To facilitate a cleaner modeling pipeline, the Credit_Score column was moved to the last column in the dataset after transformation. <br>


```{r data_cleaning_and_transformation, message=FALSE, warning=FALSE}
```{r data_cleaning_and_transformation, message=FALSE, warning=FALSE}
# Load the dataset
score_dat <- read.csv("Score.csv", stringsAsFactors = FALSE)

# Drop irrelevant columns if they exist
cols_to_drop <- c("ID", "Customer_ID", "SSN", "Name")
score_dat <- score_dat %>% select(-any_of(cols_to_drop))

# Clean numeric fields that may contain symbols or text
cols_to_clean <- c(
  "Age", "Annual_Income", "Monthly_Inhand_Salary", "Num_of_Loan",
  "Num_of_Delayed_Payment", "Changed_Credit_Limit", "Outstanding_Debt",
  "Amount_invested_monthly", "Monthly_Balance", "Credit_History_Age",
  "Num_Credit_Inquiries", "Total_EMI_per_month", "Delay_from_due_date"
)
cols_to_clean <- intersect(cols_to_clean, names(score_dat))

score_dat[cols_to_clean] <- lapply(score_dat[cols_to_clean], function(x) {
  as.numeric(gsub("[^0-9.]", "", as.character(x)))
})

# Ensure Credit_Score is a factor
if ("Credit_Score" %in% names(score_dat)) {
  score_dat$Credit_Score <- factor(score_dat$Credit_Score, levels = c("Good", "Poor", "Standard"))
}

# Handle categorical variables safely
cat_vars <- c("Payment_of_Min_Amount", "Credit_Mix")
cat_vars_existing <- intersect(cat_vars, names(score_dat))

if (length(cat_vars_existing) > 0) {
  score_dat <- score_dat %>%
    mutate(across(all_of(cat_vars_existing), as.factor))
  
  one_hot <- fastDummies::dummy_cols(score_dat[cat_vars_existing],
                                     select_columns = cat_vars_existing,
                                     remove_selected_columns = TRUE)
  
  score_dat <- score_dat %>% select(-any_of(cat_vars_existing))
  score_dat_dp <- cbind(score_dat, one_hot)
} else {
  score_dat_dp <- score_dat
}
```



## Exploratory Data Analysis (EDA)

### A bar chart shows the class distribution for Credit_Score, confirming class imbalance (an issue relevant for model training).
```{r}
ggplot(score_dat, aes(x = Credit_Score, fill = Credit_Score)) +
  geom_bar() +
  labs(
    title = "Class Distribution of Credit_Score",
    x = "Credit Score Category",
    y = "Count"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "none"
  )
```
The bar chart above illustrates the distribution of the target variable Credit_Score across three categories: Good, Poor, and Standard. It clearly shows a class imbalance, with the Standard category being the most frequent, followed by Poor, and then Good.<br>
This imbalance is important to address during model training, as standard machine learning algorithms may become biased toward the majority class. Failure to handle class imbalance can result in poor predictive performance for underrepresented classes. Therefore, techniques such as resampling, class weighting, or balanced evaluation metrics (e.g., F1-score, confusion matrix) are considered later in the modeling pipeline.

### Correlation matrix was computed and visualised to observe relationships between numeric features.
```{r }
# Extract only numeric columns
numeric_data <- score_dat %>%
  select(where(is.numeric))

# Add Credit_Score as numeric (for correlation purpose only)
numeric_data$Credit_Score <- as.numeric(score_dat$Credit_Score)

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs")

# Replace NA with 0 (safer for plotting than dropping all rows)
cor_matrix[is.na(cor_matrix)] <- 0

# Plot heatmap
p <- ggcorrplot(
  cor_matrix,
  type = "lower",
  lab = TRUE,
  lab_size = 3.5,
  hc.order = TRUE,
  method = "square",
  colors = c("blue", "white", "red"),
  outline.color = "gray90"
) +
  ggtitle("Correlation Heatmap of Dataset Features (Including Credit Score)") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)
  )

# Save high-resolution heatmap
ggsave("correlation_heatmap.png", plot = p, width = 12, height = 10, dpi = 300)

```

```{r include_correlation_heatmap, echo=FALSE, fig.align='center', out.width='100%'}
knitr::include_graphics("correlation_heatmap.png")
```

The heatmap above displays pairwise Pearson correlation coefficients among all numeric variables in the dataset, including the target variable Credit_Score (encoded as an ordinal factor). This visualization helps identify relationships between features, which is crucial for both feature selection and model interpretability.<br>
Key observations:<br>
Strong positive correlations are observed among financial variables:<br>
Annual_Income, Monthly_Balance, and Amount_invested_monthly are highly correlated (r > 0.6), suggesting potential redundancy.<br>
Num_of_Loan, Outstanding_Debt, and Num_of_Delayed_Payment also show high correlations (r > 0.6), indicating that they capture overlapping financial risk behavior.<br>
Moderate correlations are seen between Credit_Score and variables like:<br>
Num_of_Loan (r = 0.20)<br>
Credit_Utilization_Ratio (r = 0.18)<br>
Num_of_Delayed_Payment (r = 0.18)<br>
Interest_Rate and Outstanding_Debt also show mild associations with credit score.<br>
These patterns suggest that payment behavior and borrowing volume are more predictive of credit score than income-related variables, which may reflect the scoring algorithm’s emphasis on risk exposure and repayment behavior rather than just earnings.<br>
This correlation matrix guided the initial feature selection process and highlighted potential multicollinearity risks that were further addressed through Lasso regression.<br>
```{r}
# Safely handle categorical variables
cat_vars <- c("Payment_of_Min_Amount", "Credit_Mix")
cat_vars_existing <- intersect(cat_vars, names(score_dat))

if (length(cat_vars_existing) > 0) {
  # Convert to factor
  score_dat <- score_dat %>%
    mutate(across(all_of(cat_vars_existing), as.factor))
  
  # One-hot encode
  one_hot <- fastDummies::dummy_cols(score_dat,
                                     select_columns = cat_vars_existing,
                                     remove_selected_columns = TRUE)
  
  # Drop original categorical columns
  score_dat <- score_dat %>% select(-any_of(cat_vars_existing))
  
  # Combine final dataset
  score_dat_dp <- cbind(score_dat, one_hot %>% select(-any_of(names(score_dat))))
} else {
  score_dat_dp <- score_dat  # No categorical columns found; skip encoding
}

```

# Lasso Regression
Lasso regression is to perform feature slection by penalising less relevant variables and shrinking their coefficients to zero.
Process: <br>
- Split the data into training and test sets.
- Use cv.glmnet() for cross validation to find the optimal lambda.
- Predict and compute accuracy on the test set.
Objective: <br>
To reduce dimensionality and focus on the most predictive features, setting the stage for cleaner mopdeling downstream.

```{r libraries}
set.seed(123)
inTrain_lasso <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_lasso <- score_dat_dp[inTrain_lasso, ]
test_lasso <- score_dat_dp[-inTrain_lasso, ]
x_train <- model.matrix(Credit_Score ~ ., data = train_lasso)[, -1]
x_test  <- model.matrix(Credit_Score ~ ., data = test_lasso)[, -1]
y_train <- train_lasso$Credit_Score
y_test  <- test_lasso$Credit_Score

lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, family = "multinomial")
bestlam <- lasso_cv$lambda.min
lasso_preds <- predict(lasso_cv, s = bestlam, newx = x_test, type = "class")
lasso_acc <- mean(lasso_preds == y_test)
print(paste("Lasso Accuracy:", round(lasso_acc * 100, 2), "%"))
```
Based on the Lasso Regression results, the model achieved a test accuracy of 64.29%, confirming moderate predictive power while effectively reducing dimensionality. The selected features with non-zero coefficients included: Num_of_Loan, Num_of_Delayed_Payment, Interest_Rate, Credit_Utilization_Ratio, and Amount_invested_monthly. These align closely with insights from the earlier correlation heatmap, which highlighted these variables as having the strongest positive relationships with Credit_Score. Notably, income-related features such as Annual_Income and Monthly_Balance, despite being highly correlated with each other, showed weak correlation with Credit_Score and were excluded by Lasso—reinforcing the conclusion that credit behavior and repayment history are more predictive of creditworthiness than income alone.




# Forward Stepwise Selection (Multinomial Logistic Regression)
This method iteratively selects the best performing features based on accuracy improvements.<br>
Process: <br>
- Starts with no features and adds one at a time based on which feature gives the highest accuracy gain on a test set. <br>
- multinom() is used to train a model on the selected features.
Objective: <br>
This technique provides a human-interpretable subset of features, complementing Lasso's more automated selection, it ensures model simplicity and explains why certain variables are preditive.<br>

```{r lasso regression}
set.seed(123)
inTrain_fs <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_fs <- score_dat_dp[inTrain_fs, ]
test_fs  <- score_dat_dp[-inTrain_fs, ]
cls.train <- train_fs$Credit_Score
cls.test  <- test_fs$Credit_Score
train_fs <- train_fs %>% select(-Credit_Score)
test_fs  <- test_fs %>% select(-Credit_Score)

selectFeatureMultiClass <- function(train, test, cls.train, cls.test, features) {
  current.highest.accuracy <- 0
  selected.i <- NULL
  remaining <- setdiff(colnames(train), features)
  for (f in remaining) {
    selected.features <- c(features, f)
    train.sub <- data.frame(train[, selected.features, drop = FALSE], Credit_Score = cls.train)
    test.sub  <- data.frame(test[, selected.features, drop = FALSE], Credit_Score = cls.test)
    acc <- 0
    tryCatch({
      model <- multinom(Credit_Score ~ ., data = train.sub, trace = FALSE)
      preds <- predict(model, newdata = test.sub)
      acc <- mean(preds == cls.test)
    }, error = function(e) {})
    if (acc > current.highest.accuracy) {
      current.highest.accuracy <- acc
      selected.i <- f
    }
  }
  return(selected.i)
}

features.direct <- NULL
for (i in 1:5) {
  selected.i <- selectFeatureMultiClass(train_fs, test_fs, cls.train, cls.test, features.direct)
  if (is.null(selected.i)) break
  features.direct <- c(features.direct, selected.i)
}
print(paste("Top 5 Forward Selected Features:", paste(features.direct, collapse = ", ")))

formula_forward <- as.formula(paste("Credit_Score ~", paste(features.direct, collapse = "+")))
logit_model <- multinom(formula_forward, data = data.frame(train_fs[, features.direct], Credit_Score = cls.train))
logit_preds <- predict(logit_model, newdata = test_fs)
conf_matrix_logit <- confusionMatrix(logit_preds, cls.test)
print(conf_matrix_logit)
```

# Random Forest (with Feature importance)
Random forest builds multiple decision trees and aggregate their predictions, a full model is trained, and feature importance is extracted, top five features are used to train a leaner model, then accuracy and confusion matrix are reported.<br>
Objective: <br>
- More accurate and robust than linear models.
- The feature importance results also validate earlier selections from Lasso and Forward Selection.

```{r randomforest}
set.seed(42)

inTrain_rf <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_rf <- score_dat_dp[inTrain_rf, ]
test_rf  <- score_dat_dp[-inTrain_rf, ]

rf_model <- randomForest(Credit_Score ~ ., data = train_rf, ntree = 100, importance = TRUE)
rf_pred <- predict(rf_model, newdata = test_rf)

rf_acc <- mean(rf_pred == test_rf$Credit_Score)
print(paste("Random Forest Accuracy:", round(rf_acc * 100, 2), "%"))

conf_matrix_rf <- confusionMatrix(rf_pred, test_rf$Credit_Score)
print(conf_matrix_rf)
```

## HEATMAP

```{r heatmap}
conf_df <- as.data.frame(conf_matrix_rf$table)

ggplot(conf_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  labs(title = "Confusion Matrix Heatmap for Random Forest",
       x = "True Class",
       y = "Predicted Class") +
  theme_minimal()
```

# XGBoost
XGBoost is a high-performance boosting algorithm. It builds trees sequentially, improving errors from previous trees.<br>
Process: <br>
- Convert dataset to numeric matrix format.
- Use xgb.train() to fit the model.
- Extract top five features and retrain a reduced model.
- Evaluate accuracy and display confusion matrix.
XGBoost tends to be more efficient and often outperforms Random Forest in structured data. Its feature importance offers another validation path for feature selection.<br>

```{r xgboost}
set.seed(100)

inTrain_xgb <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_xgb <- score_dat_dp[inTrain_xgb, ]
test_xgb  <- score_dat_dp[-inTrain_xgb, ]

x_train <- model.matrix(Credit_Score ~ ., train_xgb)[, -1]
x_test  <- model.matrix(Credit_Score ~ ., test_xgb)[, -1]
y_train <- as.numeric(train_xgb$Credit_Score) - 1
y_test  <- as.numeric(test_xgb$Credit_Score) - 1

dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest  <- xgb.DMatrix(data = x_test, label = y_test)

params <- list(objective = "multi:softmax", num_class = 3, eval_metric = "mlogloss")
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, verbose = 0)

xgb_pred <- predict(xgb_model, dtest)
xgb_acc <- mean(xgb_pred == y_test)
print(paste("XGBoost Accuracy:", round(xgb_acc * 100, 2), "%"))

conf_matrix_xgb <- confusionMatrix(factor(xgb_pred), factor(y_test))
print(conf_matrix_xgb)
```
# Conclusion
Random Forest performed best (~78% accuracy), showing strong class-handling capability.
XGBoost (~74%) showed efficiency and potential with hyperparameter tuning.
Lasso and Forward Selection improved model simplicity and interpretability.
Together, the full pipeline reflects a professional-grade data science workflow from EDA to evaluation.
  

