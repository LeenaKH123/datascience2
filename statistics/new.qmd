---
title: "FinalProjectReport"
author: "Group 2"
format:
    html:
      code-fold: true
      code-line-numbers: true
      number-sections: true
      number-depth: 3
      code-tools: true
      embed-resources: true
---

# Overview of Problem

The goal of this project is to build a multi-class classification model to predict a customer’s Credit Score (Poor, Standard, Good) using demographic and financial behavior data. This can support automated risk assessment in financial institutions and reduce manual workload.

# About Dataset

  **Source   :** Kaggle (https://www.kaggle.com/code/sudhanshu2198/multi-class-credit-score-classification)  
  **Data set :** score.csv  
  **Samples  :** approximate 100,000  
  **Features :** Mix of numeric and categorical  
  **Target   :** Credit_Score (Poor, Standard, Good)  
  **Key Issues :** Outliers, noisy strings, class imbalance  
                   Missing Values ~1% overall; highest in Credit_History_Age, Monthly_Inhand_Salary

## Key Features and Description

- **Age**: Age of the person  
- **Annual_Income**: Annual income of the person  
- **Monthly_Inhand_Salary**: Monthly base salary  
- **Num_Bank_Accounts**: Number of bank accounts  
- **Num_Credit_Card**: Number of credit cards  
- **Interest_Rate**: Credit card interest rate  
- **Num_of_Loan**: Number of bank loans  
- **Delay_from_due_date**: Avg. days delayed  
- **Num_of_Delayed_Payment**: Avg. payments delayed  
- **Changed_Credit_Limit**: % change in credit limit  
- **Num_Credit_Inquiries**: Credit card inquiries  
- **Credit_Mix**: Credit type classification  
- **Outstanding_Debt**: Remaining debt  
- **Credit_Utilization_Ratio**: Utilization ratio  
- **Credit_History_Age**: Age of credit history  
- **Payment_of_Min_Amount**: Whether minimum amount was paid  
- **Total_EMI_per_month**: Monthly EMI payments  
- **Amount_invested_monthly**: Monthly investments  
- **Monthly_Balance**: Monthly balance  
- **Credit_Score** : Target Variable — Poor, Standard, Good  

# Challenges of the project

**a)** Huge dataset (~100K obs, 20+ features), demanding computational resources for ML models & tuning.  
**b)** Multi-class classification (Poor, Standard, Good), whereas most algorithms natively support binary.  

```{r libs}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(recipes))
suppressPackageStartupMessages(library(fastDummies))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(nnet))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggcorrplot))
options(warn = -1)  # to suppress warnings during rendering
```

Data Preparation & EDA


```{r}
score_dat <- read.csv("Score.csv", stringsAsFactors = TRUE)
score_dat <- score_dat %>% select(-Credit_Score, Credit_Score)
```

- EDA

```{r }
# Prepare numeric data for correlation matrix
numeric_data <- score_dat %>%
  mutate(across(where(is.character), as.numeric)) %>%
  mutate(across(where(is.factor), as.numeric))

numeric_data$Credit_Score <- as.numeric(score_dat$Credit_Score)

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs")

# Plot the correlation heatmap
ggcorrplot(
  cor_matrix,
  hc.order = TRUE,
  type = "lower",
  lab = TRUE,
  lab_size = 3,
  method = "square",
  colors = c("blue", "white", "red"),
  outline.color = "gray90",
  title = "Correlation Heatmap of Features (Including Credit_Score)"
) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
  )

```
```{r }
score_dat %>%
  count(Credit_Score) %>%
  mutate(Percent = round(n / sum(n) * 100, 1)) %>%
  ggplot(aes(x = Credit_Score, y = n, fill = Credit_Score)) +
  geom_col() + geom_text(aes(label = paste0(n, " (", Percent, "%)")), vjust = -0.5) +
  theme_minimal()
```


```{r}
score_dat$Credit_Score <- factor(score_dat$Credit_Score, levels = c("Good", "Poor", "Standard"))
cat_vars <- c("Payment_of_Min_Amount", "Credit_Mix")
one_hot <- dummy_cols(score_dat[cat_vars], select_columns = cat_vars, remove_selected_columns = TRUE)
score_dat <- score_dat %>% select(-one_of(cat_vars))
score_dat_dp <- cbind(score_dat, one_hot)
```

- Feature Engineering

Lasso Regression

```{r libraries}
set.seed(123)
inTrain_lasso <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_lasso <- score_dat_dp[inTrain_lasso, ]
test_lasso <- score_dat_dp[-inTrain_lasso, ]
x_train <- model.matrix(Credit_Score ~ ., data = train_lasso)[, -1]
x_test  <- model.matrix(Credit_Score ~ ., data = test_lasso)[, -1]
y_train <- train_lasso$Credit_Score
y_test  <- test_lasso$Credit_Score

lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, family = "multinomial")
bestlam <- lasso_cv$lambda.min
lasso_preds <- predict(lasso_cv, s = bestlam, newx = x_test, type = "class")
lasso_acc <- mean(lasso_preds == y_test)
print(paste("Lasso Accuracy:", round(lasso_acc * 100, 2), "%"))
```

Forward Stepwise Selection

```{r lasso regression}
set.seed(123)
inTrain_fs <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_fs <- score_dat_dp[inTrain_fs, ]
test_fs  <- score_dat_dp[-inTrain_fs, ]
cls.train <- train_fs$Credit_Score
cls.test  <- test_fs$Credit_Score
train_fs <- train_fs %>% select(-Credit_Score)
test_fs  <- test_fs %>% select(-Credit_Score)

selectFeatureMultiClass <- function(train, test, cls.train, cls.test, features) {
  current.highest.accuracy <- 0
  selected.i <- NULL
  remaining <- setdiff(colnames(train), features)
  for (f in remaining) {
    selected.features <- c(features, f)
    train.sub <- data.frame(train[, selected.features, drop = FALSE], Credit_Score = cls.train)
    test.sub  <- data.frame(test[, selected.features, drop = FALSE], Credit_Score = cls.test)
    acc <- 0
    tryCatch({
      model <- multinom(Credit_Score ~ ., data = train.sub, trace = FALSE)
      preds <- predict(model, newdata = test.sub)
      acc <- mean(preds == cls.test)
    }, error = function(e) {})
    if (acc > current.highest.accuracy) {
      current.highest.accuracy <- acc
      selected.i <- f
    }
  }
  return(selected.i)
}

features.direct <- NULL
for (i in 1:5) {
  selected.i <- selectFeatureMultiClass(train_fs, test_fs, cls.train, cls.test, features.direct)
  if (is.null(selected.i)) break
  features.direct <- c(features.direct, selected.i)
}
print(paste("Top 5 Forward Selected Features:", paste(features.direct, collapse = ", ")))

formula_forward <- as.formula(paste("Credit_Score ~", paste(features.direct, collapse = "+")))
logit_model <- multinom(formula_forward, data = data.frame(train_fs[, features.direct], Credit_Score = cls.train))
logit_preds <- predict(logit_model, newdata = test_fs)
conf_matrix_logit <- confusionMatrix(logit_preds, cls.test)
print(conf_matrix_logit)
```

Random Forest

```{r randomforest}
set.seed(42)

inTrain_rf <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_rf <- score_dat_dp[inTrain_rf, ]
test_rf  <- score_dat_dp[-inTrain_rf, ]

rf_model <- randomForest(Credit_Score ~ ., data = train_rf, ntree = 100, importance = TRUE)
rf_pred <- predict(rf_model, newdata = test_rf)

rf_acc <- mean(rf_pred == test_rf$Credit_Score)
print(paste("Random Forest Accuracy:", round(rf_acc * 100, 2), "%"))

conf_matrix_rf <- confusionMatrix(rf_pred, test_rf$Credit_Score)
print(conf_matrix_rf)
```

HEATMAP

```{r heatmap}
conf_df <- as.data.frame(conf_matrix_rf$table)

ggplot(conf_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  labs(title = "Confusion Matrix Heatmap for Random Forest",
       x = "True Class",
       y = "Predicted Class") +
  theme_minimal()
```

XGBoost
```{r xgboost}
set.seed(100)

inTrain_xgb <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_xgb <- score_dat_dp[inTrain_xgb, ]
test_xgb  <- score_dat_dp[-inTrain_xgb, ]

x_train <- model.matrix(Credit_Score ~ ., train_xgb)[, -1]
x_test  <- model.matrix(Credit_Score ~ ., test_xgb)[, -1]
y_train <- as.numeric(train_xgb$Credit_Score) - 1
y_test  <- as.numeric(test_xgb$Credit_Score) - 1

dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest  <- xgb.DMatrix(data = x_test, label = y_test)

params <- list(objective = "multi:softmax", num_class = 3, eval_metric = "mlogloss")
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, verbose = 0)

xgb_pred <- predict(xgb_model, dtest)
xgb_acc <- mean(xgb_pred == y_test)
print(paste("XGBoost Accuracy:", round(xgb_acc * 100, 2), "%"))

conf_matrix_xgb <- confusionMatrix(factor(xgb_pred), factor(y_test))
print(conf_matrix_xgb)
```
Conclusion

    Random Forest performed the best (~78% accuracy), with robust class handling.

    XGBoost came close with ~74% accuracy and faster execution.

    Lasso and Forward Selection helped in dimensionality reduction and feature insight.

    The analysis confirms that robust machine learning models can effectively automate credit scoring in financial services.

