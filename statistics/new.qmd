---
title: "FinalProjectReport"
author: "Group 2"
format:
    html:
      code-fold: true
      code-line-numbers: true
      number-sections: true
      number-depth: 3
      code-tools: true
      embed-resources: true
---

<!-- 
 Overview of the Problem

The objective of the project is to build a multi-class classification model that predicts an individual's credit score—categorized into Good, Standard, or Poor—based on various demographic and financial factors.
This task has practical applications in the financial sector, especially for automated credit scoring and risk assessment. By replacing manual evaluations with predictive analytics, institutions can streamline loan approvals and minimize default risk. 
-->

The goal of this project is to build a multi-class classification model to predict a customer’s Credit Score (Poor, Standard, Good) using demographic and financial behavior data. This can support automated risk assessment in financial institutions and reduce manual workload.


# About Dataset

  **Source   :** Kaggle (https://www.kaggle.com/code/sudhanshu2198/multi-class-credit-score-classification)  
  **Data set :** score.csv  
  **Samples  :** approximate 100,000  
  **Features :** Mix of numeric and categorical  
  **Target   :** Credit_Score (Poor, Standard, Good)  
  **Key Issues :** Outliers, noisy strings, class imbalance  
                   Missing Values ~1% overall; highest in Credit_History_Age, Monthly_Inhand_Salary

## Key Features and Description

- **Age**: Age of the person  
- **Annual_Income**: Annual income of the person  
- **Monthly_Inhand_Salary**: Monthly base salary  
- **Num_Bank_Accounts**: Number of bank accounts  
- **Num_Credit_Card**: Number of credit cards  
- **Interest_Rate**: Credit card interest rate  
- **Num_of_Loan**: Number of bank loans  
- **Delay_from_due_date**: Avg. days delayed  
- **Num_of_Delayed_Payment**: Avg. payments delayed  
- **Changed_Credit_Limit**: % change in credit limit  
- **Num_Credit_Inquiries**: Credit card inquiries  
- **Credit_Mix**: Credit type classification  
- **Outstanding_Debt**: Remaining debt  
- **Credit_Utilization_Ratio**: Utilization ratio  
- **Credit_History_Age**: Age of credit history  
- **Payment_of_Min_Amount**: Whether minimum amount was paid  
- **Total_EMI_per_month**: Monthly EMI payments  
- **Amount_invested_monthly**: Monthly investments  
- **Monthly_Balance**: Monthly balance  
- **Credit_Score** : Target Variable — Poor, Standard, Good  

# Challenges of the project

## Huge dataset (~100K obs, 20+ features), demanding computational resources for ML models & tuning.
This dataset contains around 100,000 records and over 20 features spanning numerical and categorical types. Processing this volume of data creates several computational challenges.
 - Memory Load: Training complex models like Random Forest and XGBoost on this scale can be resource-intensive, especially during cross-validation or hyperparameter tuning.<br>
 - Preprocessing Time: Feature engineering steps such as one-hot encoding, scaling, and correlation computation become time-consuming on large data.<br>
 - Model Execution Time: Algorithms with internal loops (e.g., forward selection or cross-validated Lasso) can take considerable time to converge.<br>

How We Addressed It:<br>
- Used efficient R packages like glmnet, xgboost, and randomForest, all optimized for handling large data. <br>
- Splited the data into a 60/40 train-test split instead of k-fold cross-validation in most steps to reduce execution time.<br>
- For feature selection, used Lasso Regression (automated) and Forward Stepwise Selection (manual loop) to reduce dimensionality early—so only key features are passed into final models.<br>
- We avoided fitting redundant or overly complex models by extracting and reusing key features across models (e.g., RF, XGBoost).<br>


## Multi-class classification (Poor, Standard, Good), whereas most algorithms natively support binary. 
The Credit_Score target variable has three levels: Good, Standard, and Poor. Most standard classification algorithms (e.g., logistic regression) are designed for binary outcomes.

Specific Issues:<br>
  - Some classifiers (e.g., glmnet) require special configuration for multinomial output.<br>
  - Interpreting confusion matrices is more complex with three categories.<br>
  - Class imbalance can reduce sensitivity to minority classes.<br>

How it was addressed:<br>
  - Set family = "multinomial" in glmnet to allow multi-class logistic regression.
  - Used multinom() from the nnet package, which supports multi-class output.
  - Ensemble methods like Random Forest and XGBoost naturally support multi-class classification when properly configured (e.g., num_class = 3 for XGBoost).
  - We converted categorical labels into ordered factors and integer values, ensuring all models could parse them correctly.
  - Confusion matrices were visualized with heatmaps to make multi-class evaluation easier and more intuitive.

```{r libs}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(recipes))
suppressPackageStartupMessages(library(fastDummies))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(nnet))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggcorrplot))
options(warn = -1)  # to suppress warnings during rendering
```

# Data Preparation & EDA
## Cleaning & Transformation
The original dataset contained several quality issues that had to be addressed before modeling. Below are the specific cleaning and transformation steps:

1. String Noise Removal and Numeric Conversion: <br>
The following columns contained characters like commas (,) or string patterns (e.g., "Rs.", "days") that prevented numeric interpretation. These were removed using regular expressions, and the columns were then converted to numeric:<br>

- Age
- Annual_Income
- Monthly_Inhand_Salary
- Num_Bank_Accounts
- Num_Credit_Card
- Interest_Rate 
- Num_of_Loan 
- Delay_from_due_date 
- Num_of_Delayed_Payment 
- Changed_Credit_Limit 
- Num_Credit_Inquiries 
- Outstanding_Debt 
- Credit_Utilization_Ratio 
- Credit_History_Age 
- Total_EMI_per_month 
- Amount_invested_monthly 
- Monthly_Balance
Each of these was cleaned using gsub("[^0-9.]", "", ...) and converted using as.numeric().

2. Missing Value Imputation: <br>
After converting the above fields to numeric:<br>
- Numeric Fields: Missing values were imputed using median to avoid skew from outliers.
- Categorical Fields: Missing values were filled using mode imputation, where the most frequently occurring value is assigned.
This was applied across all columns using column-wise checks.<br>

3. Categorical Variable Encoding: <br>
The following categorical columns were encoded as factors and later transformed into dummy variables:<br>
- Payment_of_Min_Amount. 
- Credit_Mix
These were transformed using one-hot encoding via the fastDummies package, allowing them to be used in machine learning algorithms that don’t handle categorical inputs natively.

4. Target Variable Formatting: <br>
The target variable:
- Credit_Score — originally a string ("Good", "Standard", "Poor") — wa converted to a factor. The factor levels were ordered as: "Good", "Poor", "Standard" to ensure consistent interpretation in multi-class classification models.<br>

5. Column Reordering: <br>
To facilitate a cleaner modeling pipeline, the Credit_Score column was moved to the last column in the dataset after transformation. <br>

```{r}
# Load raw dataset
score_dat <- read.csv("Score.csv", stringsAsFactors = FALSE)

# Fields that need numeric conversion due to extra characters
cols_to_clean <- c("Age", "Annual_Income", "Monthly_Inhand_Salary",
                   "Num_Bank_Accounts", "Num_Credit_Card", "Interest_Rate",
                   "Num_of_Loan", "Delay_from_due_date", "Num_of_Delayed_Payment",
                   "Changed_Credit_Limit", "Num_Credit_Inquiries", "Outstanding_Debt",
                   "Credit_Utilization_Ratio", "Credit_History_Age",
                   "Total_EMI_per_month", "Amount_invested_monthly", "Monthly_Balance")

# Remove non-numeric characters (e.g., commas, currency symbols)
score_dat[cols_to_clean] <- lapply(score_dat[cols_to_clean], function(x) {
  as.numeric(gsub("[^0-9.]", "", as.character(x)))
})

# Median imputation for numeric fields
median_impute <- function(x) {
  x[is.na(x)] <- median(x, na.rm = TRUE)
  return(x)
}
score_dat[cols_to_clean] <- lapply(score_dat[cols_to_clean], median_impute)

# Handle categorical variables
cat_vars <- c("Payment_of_Min_Amount", "Credit_Mix")
score_dat[cat_vars] <- lapply(score_dat[cat_vars], as.factor)

# Mode imputation for categorical fields
mode_impute <- function(x) {
  mode_val <- names(sort(table(x), decreasing = TRUE))[1]
  x[is.na(x)] <- mode_val
  return(x)
}
score_dat[cat_vars] <- lapply(score_dat[cat_vars], mode_impute)

# One-hot encoding for categorical variables
library(fastDummies)
score_dat <- dummy_cols(score_dat, select_columns = cat_vars, remove_selected_columns = TRUE)

# Convert target to factor
score_dat$Credit_Score <- factor(score_dat$Credit_Score, levels = c("Good", "Poor", "Standard"))

# Move Credit_Score to last column for readability
score_dat <- score_dat %>% select(-Credit_Score, Credit_Score)

```


## Exploratory Data Analysis (EDA)
- Correlation matrix was computed and visualised to observe relationships between numeric features.
- A bar chart shows the class distribution for Credit_Score, confirming class imbalance (an issue relevant for model training).
- These visualizations guide which features might be important and which require scaling or imputation.
```{r }
library(ggcorrplot)

# Prepare clean numeric subset for correlation matrix
numeric_data <- score_dat %>%
  select(-Credit_Score) %>%
  mutate(across(everything(), as.numeric))

# Add Credit_Score as numeric for inclusion in correlation
numeric_data$Credit_Score <- as.numeric(score_dat$Credit_Score)

# Compute correlation
cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs")

# Generate plot
p <- ggcorrplot(
  cor_matrix,
  type = "lower",
  lab = TRUE,
  lab_size = 3.5,
  hc.order = TRUE,
  method = "square",
  colors = c("blue", "white", "red"),
  outline.color = "gray90"
) +
  ggtitle("Correlation Heatmap of Dataset Features (Including Credit Score)") +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)
  )

# Save image
ggsave("correlation_heatmap.png", plot = p, width = 12, height = 10, dpi = 300)

```

```{r include_correlation_heatmap, echo=FALSE, fig.align='center', out.width='100%'}
knitr::include_graphics("correlation_heatmap.png")
```


# Lasso Regression
Lasso regression is to perform feature slection by penalising less relevant variables and shrinking their coefficients to zero.
Process: <br>
- Split the data into training and test sets.
- Use cv.glmnet() for cross validation to find the optimal lambda.
- Predict and compute accuracy on the test set.
Objective: <br>
To reduce dimensionality and focus on the most predictive features, setting the stage for cleaner mopdeling downstream.

```{r libraries}
set.seed(123)
inTrain_lasso <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_lasso <- score_dat_dp[inTrain_lasso, ]
test_lasso <- score_dat_dp[-inTrain_lasso, ]
x_train <- model.matrix(Credit_Score ~ ., data = train_lasso)[, -1]
x_test  <- model.matrix(Credit_Score ~ ., data = test_lasso)[, -1]
y_train <- train_lasso$Credit_Score
y_test  <- test_lasso$Credit_Score

lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, family = "multinomial")
bestlam <- lasso_cv$lambda.min
lasso_preds <- predict(lasso_cv, s = bestlam, newx = x_test, type = "class")
lasso_acc <- mean(lasso_preds == y_test)
print(paste("Lasso Accuracy:", round(lasso_acc * 100, 2), "%"))
```

# Forward Stepwise Selection (Multinomial Logistic Regression)
This method iteratively selects the best performing features based on accuracy improvements.<br>
Process: <br>
- Starts with no features and adds one at a time based on which feature gives the highest accuracy gain on a test set. <br>
- multinom() is used to train a model on the selected features.
Objective: <br>
This technique provides a human-interpretable subset of features, complementing Lasso's more automated selection, it ensures model simplicity and explains why certain variables are preditive.<br>

```{r lasso regression}
set.seed(123)
inTrain_fs <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_fs <- score_dat_dp[inTrain_fs, ]
test_fs  <- score_dat_dp[-inTrain_fs, ]
cls.train <- train_fs$Credit_Score
cls.test  <- test_fs$Credit_Score
train_fs <- train_fs %>% select(-Credit_Score)
test_fs  <- test_fs %>% select(-Credit_Score)

selectFeatureMultiClass <- function(train, test, cls.train, cls.test, features) {
  current.highest.accuracy <- 0
  selected.i <- NULL
  remaining <- setdiff(colnames(train), features)
  for (f in remaining) {
    selected.features <- c(features, f)
    train.sub <- data.frame(train[, selected.features, drop = FALSE], Credit_Score = cls.train)
    test.sub  <- data.frame(test[, selected.features, drop = FALSE], Credit_Score = cls.test)
    acc <- 0
    tryCatch({
      model <- multinom(Credit_Score ~ ., data = train.sub, trace = FALSE)
      preds <- predict(model, newdata = test.sub)
      acc <- mean(preds == cls.test)
    }, error = function(e) {})
    if (acc > current.highest.accuracy) {
      current.highest.accuracy <- acc
      selected.i <- f
    }
  }
  return(selected.i)
}

features.direct <- NULL
for (i in 1:5) {
  selected.i <- selectFeatureMultiClass(train_fs, test_fs, cls.train, cls.test, features.direct)
  if (is.null(selected.i)) break
  features.direct <- c(features.direct, selected.i)
}
print(paste("Top 5 Forward Selected Features:", paste(features.direct, collapse = ", ")))

formula_forward <- as.formula(paste("Credit_Score ~", paste(features.direct, collapse = "+")))
logit_model <- multinom(formula_forward, data = data.frame(train_fs[, features.direct], Credit_Score = cls.train))
logit_preds <- predict(logit_model, newdata = test_fs)
conf_matrix_logit <- confusionMatrix(logit_preds, cls.test)
print(conf_matrix_logit)
```

# Random Forest (with Feature importance)
Random forest builds multiple decision trees and aggregate their predictions, a full model is trained, and feature importance is extracted, top five features are used to train a leaner model, then accuracy and confusion matrix are reported.<br>
Objective: <br>
- More accurate and robust than linear models.
- The feature importance results also validate earlier selections from Lasso and Forward Selection.

```{r randomforest}
set.seed(42)

inTrain_rf <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_rf <- score_dat_dp[inTrain_rf, ]
test_rf  <- score_dat_dp[-inTrain_rf, ]

rf_model <- randomForest(Credit_Score ~ ., data = train_rf, ntree = 100, importance = TRUE)
rf_pred <- predict(rf_model, newdata = test_rf)

rf_acc <- mean(rf_pred == test_rf$Credit_Score)
print(paste("Random Forest Accuracy:", round(rf_acc * 100, 2), "%"))

conf_matrix_rf <- confusionMatrix(rf_pred, test_rf$Credit_Score)
print(conf_matrix_rf)
```

## HEATMAP

```{r heatmap}
conf_df <- as.data.frame(conf_matrix_rf$table)

ggplot(conf_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  labs(title = "Confusion Matrix Heatmap for Random Forest",
       x = "True Class",
       y = "Predicted Class") +
  theme_minimal()
```

# XGBoost
XGBoost is a high-performance boosting algorithm. It builds trees sequentially, improving errors from previous trees.<br>
Process: <br>
- Convert dataset to numeric matrix format.
- Use xgb.train() to fit the model.
- Extract top five features and retrain a reduced model.
- Evaluate accuracy and display confusion matrix.
XGBoost tends to be more efficient and often outperforms Random Forest in structured data. Its feature importance offers another validation path for feature selection.<br>

```{r xgboost}
set.seed(100)

inTrain_xgb <- createDataPartition(score_dat_dp$Credit_Score, p = 0.6, list = FALSE)
train_xgb <- score_dat_dp[inTrain_xgb, ]
test_xgb  <- score_dat_dp[-inTrain_xgb, ]

x_train <- model.matrix(Credit_Score ~ ., train_xgb)[, -1]
x_test  <- model.matrix(Credit_Score ~ ., test_xgb)[, -1]
y_train <- as.numeric(train_xgb$Credit_Score) - 1
y_test  <- as.numeric(test_xgb$Credit_Score) - 1

dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest  <- xgb.DMatrix(data = x_test, label = y_test)

params <- list(objective = "multi:softmax", num_class = 3, eval_metric = "mlogloss")
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, verbose = 0)

xgb_pred <- predict(xgb_model, dtest)
xgb_acc <- mean(xgb_pred == y_test)
print(paste("XGBoost Accuracy:", round(xgb_acc * 100, 2), "%"))

conf_matrix_xgb <- confusionMatrix(factor(xgb_pred), factor(y_test))
print(conf_matrix_xgb)
```
# Conclusion
Random Forest performed best (~78% accuracy), showing strong class-handling capability.
XGBoost (~74%) showed efficiency and potential with hyperparameter tuning.
Lasso and Forward Selection improved model simplicity and interpretability.
Together, the full pipeline reflects a professional-grade data science workflow from EDA to evaluation.
  

